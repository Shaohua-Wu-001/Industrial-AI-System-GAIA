import json
import re
import os
from typing import Dict, List, Any, Optional, Tuple, Set
from pathlib import Path
from dataclasses import dataclass, field
from collections import defaultdict

# å·¥å…· Schema å®šç¾© (43 ç¨®å·¥å…·)

TOOL_SCHEMAS = {
    # File Operations (10 ç¨®)
    'read_pdf': {'required': ['file_path'], 'optional': ['page_numbers']},
    'read_csv': {'required': ['file_path'], 'optional': ['encoding']},
    'read_excel': {'required': ['file_path'], 'optional': ['sheet_name']},
    'read_json': {'required': ['file_path'], 'optional': ['encoding']},
    'read_xml': {'required': ['file_path'], 'optional': ['encoding']},
    'read_text_file': {'required': ['file_path'], 'optional': ['encoding']},
    'read_docx': {'required': ['file_path'], 'optional': []},
    'read_image': {'required': ['file_path'], 'optional': []},
    'extract_zip': {'required': ['zip_path'], 'optional': ['extract_to', 'password']},
    'image_to_text': {'required': ['file_path'], 'optional': ['lang']},
    
    # Web Operations (3 ç¨®)
    'web_search': {'required': ['query'], 'optional': ['num_results']},
    'web_fetch': {'required': ['url'], 'optional': ['timeout']},
    'wikipedia_search': {'required': ['query'], 'optional': ['language', 'sentences']},
    
    # Data Processing (15 ç¨®)
    'filter_data': {'required': ['data', 'conditions'], 'optional': []},
    'compare_values': {'required': ['value1', 'value2'], 'optional': ['comparison', 'tolerance']},
    'find_in_text': {'required': ['text', 'search_terms'], 'optional': ['context_chars', 'max_results']},
    'count_occurrences': {'required': ['data', 'target'], 'optional': ['case_sensitive', 'count_type']},
    'extract_information': {'required': ['text', 'extract_type'], 'optional': ['keywords', 'pattern']},
    'deduplicate_data': {'required': ['data'], 'optional': ['key_fields']},
    'join_data': {'required': ['data1', 'data2', 'join_key'], 'optional': ['join_type']},
    'sort_data': {'required': ['data', 'sort_by'], 'optional': ['reverse']},
    'aggregate_data': {'required': ['data', 'group_by', 'aggregate_field'], 'optional': ['operation']},
    'pivot_table': {'required': ['data', 'index', 'values'], 'optional': ['aggfunc']},
    'fill_missing': {'required': ['data', 'columns'], 'optional': ['method']},
    'sample_data': {'required': ['data', 'n'], 'optional': ['random_seed']},
    'compare_data': {'required': ['data1', 'data2'], 'optional': ['comparison_type']},
    'list_operations': {'required': ['list1', 'operation'], 'optional': ['list2']},
    'validate_data': {'required': ['data', 'rules'], 'optional': []},
    
    # Calculation & Conversion (8 ç¨®)
    'calculate': {'required': ['expression'], 'optional': []},
    'unit_converter': {'required': ['value', 'from_unit', 'to_unit'], 'optional': ['unit_type']},
    'currency_converter': {'required': ['amount', 'from_currency', 'to_currency'], 'optional': []},
    'date_calculator': {'required': ['start_date'], 'optional': ['days_to_add', 'end_date', 'date_format']},
    'statistical_analysis': {'required': ['data', 'metrics'], 'optional': []},
    'correlation_analysis': {'required': ['data', 'x_column', 'y_column'], 'optional': ['method']},
    'moving_average': {'required': ['data', 'window_size'], 'optional': []},
    'geocoding': {'required': ['location', 'return_info'], 'optional': []},
    
    # Text Operations (7 ç¨®)
    'regex_search': {'required': ['text', 'pattern'], 'optional': ['return_all']},
    'string_transform': {'required': ['text', 'operation'], 'optional': []},
    'encode_decode': {'required': ['text', 'operation'], 'optional': []},
    'split_join_text': {'required': ['text', 'operation'], 'optional': ['separator']},
    'create_csv': {'required': ['data', 'filename'], 'optional': ['include_header']},
    'create_markdown': {'required': ['title', 'sections'], 'optional': ['filename']},
    'analyze_image': {'required': ['file_path'], 'optional': []},
}

# æ”¯æ´çš„ unit_type
VALID_UNIT_TYPES = ['length', 'weight', 'volume', 'temperature', 'time', 'pressure']

# åƒæ•¸åç¨±æ˜ å°„ï¼ˆéŒ¯èª¤ -> æ­£ç¢ºï¼‰
PARAM_NAME_MAPPING = {
    'extract_information': {
        'target': 'keywords',
    },
    'deduplicate_data': {
        'key': 'key_fields',
    },
}


# ============================================================
# è³‡æ–™çµæ§‹
# ============================================================

@dataclass
class ParsedStep:
    step_number: int
    original_text: str
    tool_name: Optional[str]
    arguments: Dict[str, Any]
    confidence: int  # 0-3
    intent_category: str
    extraction_method: str
    notes: List[str] = field(default_factory=list)
    is_reasoning: bool = False
    executable: bool = True
    skip_reason: Optional[str] = None
    
    def to_dict(self):
        return {
            'step_id': f'step_{self.step_number}',
            'original_text': self.original_text,
            'tool_name': self.tool_name,
            'arguments': self.arguments,
            'confidence': self.confidence,
            'intent_category': self.intent_category,
            'extraction_method': self.extraction_method,
            'notes': self.notes,
            'is_reasoning': self.is_reasoning,
            'executable': self.executable,
            'skip_reason': self.skip_reason,
            'description': self.original_text[:200]
        }


@dataclass
class ParsingContext:
    downloaded_files: List[str] = field(default_factory=list)
    fetched_urls: List[str] = field(default_factory=list)
    data_sources: Dict[int, str] = field(default_factory=dict)
    variables: Dict[str, Any] = field(default_factory=dict)
    last_calculation: Optional[str] = None
    last_data_operation: Optional[str] = None
    opened_files: List[str] = field(default_factory=list)
    recent_searches: List[str] = field(default_factory=list)


# ============================================================
# Phase 1: æ¨ç†éæ¿¾å™¨ (from v2.1)
# ============================================================

class ReasoningFilterV5:    
    # ç´”æ¨ç†å‹•è©
    REASONING_VERBS = {
        'fix', 'fixes', 'fixed', 'fixing',
        'see', 'sees', 'saw', 'seeing',
        'when', 'if', 'as',
        'note', 'noted', 'noting', 'notes',
        'check', 'checked', 'checking', 'checks',
        'verify', 'verified', 'verifying', 'verifies',
        'confirm', 'confirmed', 'confirming', 'confirms',
        'determine', 'determined', 'determining', 'determines',
        'conclude', 'concluded', 'concluding', 'concludes',
        'round', 'rounded', 'rounding', 'rounds',
        'consider', 'considered', 'considering', 'considers',
        'assume', 'assumed', 'assuming', 'assumes',
    }
    
    # UI æ“ä½œå‹•è©ï¼ˆå·²ç§»é™¤ 'click'ï¼‰
    UI_VERBS = {
        'scroll', 'scrolled', 'scrolling', 'scrolls',
        'navigate', 'navigated', 'navigating', 'navigates',
        'watch', 'watched', 'watching', 'watches',
        'listen', 'listened', 'listening', 'listens',
    }
    
    # ç‹€æ…‹è¨˜éŒ„æ¨¡å¼
    STATE_PATTERNS = [
        r'\(running tally:?\s*\d+/\d+\)',
        r'\d+/\d+\s*(?:updated|not updated)',
        r'noted?\s+its\s+',
        r'repeated?\s+steps?\s+\d+',
        r'repeat(?:ed)?\s+(?:the\s+)?(?:steps?|process|procedure)',
    ]
    
    @classmethod
    def is_reasoning(cls, text: str) -> bool:
        text_lower = text.lower().strip()
        first_word = text_lower.split()[0] if text_lower.split() else ''
        
        # æª¢æŸ¥é¦–å­—æ˜¯å¦ç‚ºæ¨ç†å‹•è©
        if first_word in cls.REASONING_VERBS:
            return True
        
        # æª¢æŸ¥æ˜¯å¦ç‚º UI æ“ä½œ
        if first_word in cls.UI_VERBS:
            return True
        
        # æª¢æŸ¥ç‹€æ…‹è¨˜éŒ„æ¨¡å¼
        for pattern in cls.STATE_PATTERNS:
            if re.search(pattern, text_lower):
                return True
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ¢ä»¶å¥
        if text_lower.startswith(('when ', 'if ', 'as ')):
            return True
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ•¸å­¸æ¨ç†
        if '"fix"' in text_lower and ('column' in text_lower or 'number' in text_lower):
            return True
        
        # æª¢æŸ¥ "we checked" æ¨¡å¼
        if text_lower.startswith('we ') and any(v in text_lower for v in ['checked', 'found', 'verified']):
            return True
        
        return False


# ============================================================
# Phase 2: Placeholder & URL é©—è­‰å™¨ (from v3)
# ============================================================

class ValidationUtilsV5:
    
    PLACEHOLDER_PATTERNS = [
        r'^<.*>$',              # <anything>
        r'<from_context>',      # <from_context>
        r'<iterate:',           # <iterate:something>
        r'<clicked:',           # <clicked:something>
        r'<link_in:',           # <link_in:something>
        r'<result:',            # <result:something>
        r'<multiple:',          # <multiple:something>
        r'<infer>',             # <infer>
        r'<new_tab>',           # <new_tab>
        r'<page:',              # <page:something>
        r'<followed:',          # <followed:something>
        r'<conversion_constant>',
    ]
    
    @staticmethod
    def is_placeholder(value: Any) -> bool:
        if not isinstance(value, str):
            return False
        
        for pattern in ValidationUtilsV5.PLACEHOLDER_PATTERNS:
            if re.search(pattern, value, re.IGNORECASE):
                return True
        
        return False
    
    @staticmethod
    def is_valid_url(url: str) -> bool:
        if ValidationUtilsV5.is_placeholder(url):
            return False
        
        # å¿…é ˆä»¥ http:// æˆ– https:// é–‹é ­
        if not url.startswith(('http://', 'https://')):
            return False
        
        # ä¸èƒ½åŒ…å« placeholder æ¨™è¨˜
        if '<' in url or '>' in url:
            return False
        
        return True
    
    @staticmethod
    def is_valid_file_path(file_path: str, data_dir: Path) -> bool:
        if ValidationUtilsV5.is_placeholder(file_path):
            return False
        
        # æª¢æŸ¥å¤šå€‹å¯èƒ½çš„è·¯å¾‘
        paths_to_check = [
            Path(file_path),
            data_dir / file_path,
            data_dir / Path(file_path).name,
        ]
        
        for path in paths_to_check:
            if path.exists() and path.is_file():
                return True
        
        return False
    
    @staticmethod
    def clean_calculate_expression(expression: str) -> Optional[str]:
        if ValidationUtilsV5.is_placeholder(expression):
            return None
        
        # ç§»é™¤å–®ä½ (g/mol, L-atm, K-mol ç­‰)
        cleaned = re.sub(r'\s*[a-zA-Z]+(/[a-zA-Z]+)*', '', expression)
        
        # ç§»é™¤ %
        cleaned = cleaned.replace('%', '/100')
        
        # ç§»é™¤å¤šé¤˜ç©ºæ ¼
        cleaned = re.sub(r'\s+', '', cleaned)
        
        # é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆè¡¨é”å¼
        try:
            eval(cleaned)
            return cleaned
        except:
            return None
    
    @staticmethod
    def fix_wikipedia_url(url: str) -> str:
        if 'wikipedia.org' not in url:
            return url
        
        # ä¿®å¾©ä¸å®Œæ•´çš„æ‹¬è™Ÿ
        open_count = url.count('(')
        close_count = url.count(')')
        
        if open_count > close_count:
            url += ')' * (open_count - close_count)
        
        return url
    
    @staticmethod
    def infer_unit_type(from_unit: str, to_unit: str) -> str:
        units_lower = (from_unit + to_unit).lower()
        
        # é‡é‡
        if any(u in units_lower for u in ['kg', 'g', 'lb', 'oz', 'ton', 'gram', 'kilogram']):
            return 'weight'
        
        # é•·åº¦
        if any(u in units_lower for u in ['m', 'km', 'cm', 'mm', 'mile', 'ft', 'inch', 'meter']):
            return 'length'
        
        # é«”ç©
        if any(u in units_lower for u in ['l', 'ml', 'gallon', 'liter', 'litre']):
            return 'volume'
        
        # æº«åº¦
        if any(u in units_lower for u in ['c', 'f', 'k', 'celsius', 'fahrenheit', 'kelvin']):
            return 'temperature'
        
        # å£“åŠ›ï¼ˆgaia_function ä¸æ”¯æ´ï¼Œä½†æ¨™è¨˜å‡ºä¾†ï¼‰
        if any(u in units_lower for u in ['psi', 'atm', 'pa', 'bar', 'pascal']):
            return 'pressure'
        
        # é è¨­
        return 'length'


# ============================================================
# Phase 3: æª”æ¡ˆæ˜ å°„ç®¡ç†å™¨ (from v3.1)
# ============================================================

class FileMapperV5:
    
    def __init__(self, tasks: List[Dict], data_dir: Path):
        self.tasks = tasks
        self.data_dir = data_dir
        self.file_map = self._build_file_map()
    
    def _build_file_map(self) -> Dict[str, str]:
        file_map = {}
        
        if not self.data_dir.exists():
            return file_map
        
        for task in self.tasks:
            task_id = task['task_id']
            file_name = task.get('file_name', '')
            
            if not file_name:
                continue
            
            # æ–¹æ³• 1: ç²¾ç¢ºåŒ¹é…
            exact_path = self.data_dir / file_name
            if exact_path.exists():
                file_map[task_id] = str(exact_path)
                continue
            
            # æ–¹æ³• 2: å‰ç¶´åŒ¹é…ï¼ˆå‰ 8 å€‹å­—å…ƒï¼‰
            prefix = file_name.split('.')[0][:8]
            for f in self.data_dir.iterdir():
                if f.is_file() and f.name.startswith(prefix):
                    file_map[task_id] = str(f)
                    break
            
            # æ–¹æ³• 3: ZIP æª”æ¡ˆè¨˜éŒ„
            if task_id not in file_map and file_name.endswith('.zip'):
                zip_path = self.data_dir / file_name
                if zip_path.exists():
                    file_map[task_id] = str(zip_path)
        
        return file_map
    
    def get_file_path(self, task_id: str) -> Optional[str]:
        return self.file_map.get(task_id)
    
    def has_file(self, task_id: str) -> bool:
        return task_id in self.file_map


# ============================================================
# Phase 4: å·¥å…·æå–å™¨ - å®Œæ•´ v2.1 è¦å‰‡
# ============================================================

class ToolExtractorV5:

    def __init__(self):
        self.rules = self._build_extraction_rules()
    
    def _build_extraction_rules(self) -> Dict[str, List[Dict]]:

        return {
            'read_pdf': [
                {
                    'pattern': r'opened?\s+(?:the\s+)?["\']?(.+?\.pdf)["\']?',
                    'confidence': 3,
                    'extract': lambda m: {'file_path': f'./data/{m.group(1)}'}
                },
                {
                    'pattern': r'opened?\s+(?:the\s+)?pdf',
                    'confidence': 2,
                    'extract': lambda m: {'file_path': './data/document.pdf'}
                },
                {
                    'pattern': r'clicked?\s+["\']?pdf',
                    'confidence': 2,
                    'extract': lambda m: {'file_path': './data/document.pdf'}
                },
            ],
            
            'read_csv': [
                {
                    'pattern': r'opened?\s+(?:the\s+)?["\']?(.+?\.csv)["\']?',
                    'confidence': 3,
                    'extract': lambda m: {'file_path': f'./data/{m.group(1)}'}
                },
                {
                    'pattern': r'(?:read|loaded?|parsed?)\s+(?:the\s+)?csv',
                    'confidence': 2,
                    'extract': lambda m: {'file_path': './data/data.csv'}
                },
            ],
            
            'read_excel': [
                {
                    'pattern': r'opened?\s+(?:the\s+)?["\']?(.+?\.xlsx?)["\']?',
                    'confidence': 3,
                    'extract': lambda m: {'file_path': f'./data/{m.group(1)}', 'sheet_name': 'Sheet1'}
                },
                {
                    'pattern': r'(?:open|opened?)\s+(?:the\s+)?spreadsheet',
                    'confidence': 2,
                    'extract': lambda m: {'file_path': './data/spreadsheet.xlsx', 'sheet_name': 'Sheet1'}
                },
            ],
            
            'read_json': [
                {
                    'pattern': r'opened?\s+(?:the\s+)?["\']?(.+?\.json(?:ld)?)["\']?',
                    'confidence': 3,
                    'extract': lambda m: {'file_path': f'./data/{m.group(1)}'}
                },
                {
                    'pattern': r'opened?\s+(?:the\s+)?(?:json|jsonld)\s+file',
                    'confidence': 2,
                    'extract': lambda m: {'file_path': './data/data.json'}
                },
            ],
            
            'read_xml': [
                {
                    'pattern': r'opened?\s+(?:the\s+)?["\']?(.+?\.xml)["\']?',
                    'confidence': 3,
                    'extract': lambda m: {'file_path': f'./data/{m.group(1)}'}
                },
                {
                    'pattern': r'(?:open|opened?)\s+(?:the\s+)?xml',
                    'confidence': 2,
                    'extract': lambda m: {'file_path': './data/data.xml'}
                },
            ],
            
            'read_image': [
                {
                    'pattern': r'opened?\s+(?:the\s+)?["\']?(.+?\.(?:png|jpg|jpeg|gif|bmp|webp))["\']?',
                    'confidence': 3,
                    'extract': lambda m: {'file_path': f'./data/{m.group(1)}'}
                },
            ],
            
            'image_to_text': [
                {
                    'pattern': r'(?:extracted?|read)\s+(?:the\s+)?text\s+from\s+(?:the\s+)?image',
                    'confidence': 2,
                    'extract': lambda m: {'file_path': './data/image.png'}
                },
            ],
            
            'extract_zip': [
                {
                    'pattern': r'(?:extracted?|unzipped?|decompressed?)\s+(?:the\s+)?["\']?(.+?\.zip)["\']?',
                    'confidence': 3,
                    'extract': lambda m: {'zip_path': f'./data/{m.group(1)}'}
                },
            ],
            
            # ========== Web Operations ==========
            'web_search': [
                {
                    'pattern': r'searched?\s+["\'](.+?)["\']',
                    'confidence': 3,
                    'extract': lambda m: {'query': m.group(1)}
                },
                {
                    'pattern': r'searched?\s+(.+?)\s+on\s+(?:google|the\s+web|internet)',
                    'confidence': 3,
                    'extract': lambda m: {'query': m.group(1).strip().strip('"\')')}
                },
                {
                    'pattern': r'searched?\s+for\s+(.+?)(?:\s+(?:in|on|and|to)|\s*$)',
                    'confidence': 2,
                    'extract': lambda m: {'query': m.group(1).strip()}
                },
                {
                    'pattern': r'^search\s+(.+?)(?:\s+and\s+open|\s*$)',
                    'confidence': 2,
                    'extract': lambda m: {'query': m.group(1).strip()}
                },
            ],
            
            'web_fetch': [
                {
                    'pattern': r'opened?\s+(https?://[^\s\)]+)',
                    'confidence': 3,
                    'extract': lambda m: {'url': m.group(1).rstrip('.,;')}
                },
                {
                    'pattern': r'clicked?\s+(?:on\s+)?(?:the\s+)?["\']?(.+?)["\']?\s+(?:link|page|button)',
                    'confidence': 2,
                    'extract': lambda m: {'url': f'<clicked:{m.group(1).strip()}>'}
                },
                {
                    'pattern': r'opened?\s+(?:the\s+)?["\'](.+?)["\']?\s+page',
                    'confidence': 2,
                    'extract': lambda m: {'url': f'<page:{m.group(1).strip()}>'}
                },
                {
                    'pattern': r'opened?\s+(?:a\s+)?new\s+tab',
                    'confidence': 1,
                    'extract': lambda m: {'url': '<new_tab>'}
                },
                {
                    'pattern': r'opened?\s+each\s+(.+?)(?:\s+|$)',
                    'confidence': 2,
                    'extract': lambda m: {'url': f'<iterate:{m.group(1).strip()}>'}
                },
                {
                    'pattern': r'(?:followed?|navigated?\s+to)\s+(?:the\s+)?(?:link|url)',
                    'confidence': 2,
                    'extract': lambda m: {'url': '<from_context>'}
                },
            ],
            
            'wikipedia_search': [
                {
                    'pattern': r'(?:searched?|looked\s+up)\s+(.+?)\s+(?:on|in)\s+wikipedia',
                    'confidence': 3,
                    'extract': lambda m: {'query': m.group(1).strip()}
                },
                {
                    'pattern': r'went\s+(?:back\s+)?to\s+(?:the\s+)?wikipedia',
                    'confidence': 2,
                    'extract': lambda m: {'query': '<infer>'}
                },
                {
                    'pattern': r'opened?\s+["\'](.+?)["\']?\s+on\s+wikipedia',
                    'confidence': 3,
                    'extract': lambda m: {'query': m.group(1).strip()}
                },
            ],
            
            # ========== Calculation Operations ==========
            'calculate': [
                {
                    'pattern': r'calculated?\s+the\s+percentage\s*\(([^)]+?)\s*%?\s*\)',
                    'confidence': 3,
                    'extract': lambda m: {'expression': self._clean_expression(m.group(1))}
                },
                {
                    'pattern': r'calculated?\s*[:\(]\s*(.+?)(?:\s*=|\s*\)|$)',
                    'confidence': 3,
                    'extract': lambda m: {'expression': self._clean_expression(m.group(1))}
                },
                {
                    'pattern': r'took\s+the\s+(?:percentage|average|sum)\s*[:\(]\s*(.+?)(?:\s*=|\s*\)|$)',
                    'confidence': 3,
                    'extract': lambda m: {'expression': self._clean_expression(m.group(1))}
                },
                {
                    'pattern': r'calculated?\s+(?:moles?|value|result)\s*:\s*(.+?)(?:\s*=|$)',
                    'confidence': 2,
                    'extract': lambda m: {'expression': self._clean_expression(m.group(1).split('=')[0])}
                },
            ],
            
            'unit_converter': [
                # Rule 1: "Converted X unit to Y unit"
                {
                    'pattern': r'converted?\s+(\d+(?:\.\d+)?)\s+([a-zA-Z]+)\s+to\s+([a-zA-Z]+)',
                    'confidence': 3,
                    'extract': lambda m: {
                        'value': float(m.group(1)),
                        'from_unit': m.group(2),
                        'to_unit': m.group(3),
                        'unit_type': '<infer>'
                    } if m.group(2).lower() not in ['usd', 'eur', 'gbp', 'jpy', 'cny'] else None
                },
                # Rule 2: "Converted X unit = Y unit" (with result shown)
                {
                    'pattern': r'converted?\s+(\d+(?:,\d{3})*(?:\.\d+)?)\s+([a-zA-Z]+)\s*(?:=|to)\s*(\d+(?:,\d{3})*(?:\.\d+)?)\s*([a-zA-Z]+)',
                    'confidence': 3,
                    'extract': lambda m: {
                        'value': float(m.group(1).replace(',', '')),
                        'from_unit': m.group(2),
                        'to_unit': m.group(4),
                        'unit_type': '<infer>'
                    } if m.group(2).lower() not in ['usd', 'eur', 'gbp', 'jpy', 'cny'] else None
                },
                # Rule 3: "Converted to X: Y Z = result" (e.g., "Converted to mL: 0.05473 L = 54")
                {
                    'pattern': r'converted?\s+to\s+([a-zA-Z]+)\s*:\s*(\d+(?:\.\d+)?)\s+([a-zA-Z]+)',
                    'confidence': 2,
                    'extract': lambda m: {
                        'value': float(m.group(2)),
                        'from_unit': m.group(3),
                        'to_unit': m.group(1),
                        'unit_type': '<infer>'
                    } if m.group(3).lower() not in ['usd', 'eur', 'gbp', 'jpy', 'cny'] else None
                },
                # Rule 4: "Converted X to Y: value * factor" (e.g., "Converted psi to atm: 15,750 * 0.068046")
                {
                    'pattern': r'converted?\s+([a-zA-Z]+)\s+to\s+([a-zA-Z]+)\s*:\s*(\d+(?:,\d{3})*(?:\.\d+)?)',
                    'confidence': 2,
                    'extract': lambda m: {
                        'value': float(m.group(3).replace(',', '')),
                        'from_unit': m.group(1),
                        'to_unit': m.group(2),
                        'unit_type': '<infer>'
                    } if m.group(1).lower() not in ['usd', 'eur', 'gbp', 'jpy', 'cny'] else None
                },
                # Rule 5: "Converted X.X unit = Y.Y unit" (decimal numbers with result)
                {
                    'pattern': r'converted?\s+(\d+(?:\.\d+)?)\s*([a-zA-Z]+)\s*=\s*(\d+(?:\.\d+)?)\s*([a-zA-Z]+)',
                    'confidence': 2,
                    'extract': lambda m: {
                        'value': float(m.group(1)),
                        'from_unit': m.group(2),
                        'to_unit': m.group(4),
                        'unit_type': '<infer>'
                    } if m.group(2).lower() not in ['usd', 'eur', 'gbp', 'jpy', 'cny'] else None
                },
            ],
            
            # ========== Data Operations ==========
            'compare_values': [
                {
                    'pattern': r'compar(?:ed?|ing)\s+(.+?)\s+(?:and|with|to)\s+(.+?)(?:\s*$|\s+(?:and|to))',
                    'confidence': 2,
                    'extract': lambda m: {
                        'value1': m.group(1).strip(),
                        'value2': m.group(2).strip()
                    }
                },
                {
                    'pattern': r'compar(?:ed?|ing)\s+(?:the\s+)?(\w+)',
                    'confidence': 1,
                    'extract': lambda m: {
                        'value1': '<from_context>',
                        'value2': '<from_context>'
                    }
                },
            ],
            
            'filter_data': [
                {
                    'pattern': r'filter(?:ed)?\s+(?:the\s+)?(?:data|rows?|records?)',
                    'confidence': 2,
                    'extract': lambda m: {
                        'data': '<from_context>',
                        'conditions': {}
                    }
                },
                {
                    'pattern': r'opened?\s+(?:the\s+)?filters?\s+and\s+set',
                    'confidence': 2,
                    'extract': lambda m: {
                        'data': '<from_context>',
                        'conditions': {}
                    }
                },
            ],
            
            'extract_information': [
                {
                    'pattern': r'found\s+(?:the\s+)?(.+?)(?:\s+(?:as|in|from|was|of)|$)',
                    'confidence': 2,
                    'extract': lambda m: {
                        'text': '<from_context>',
                        'extract_type': 'specific',
                        'keywords': [m.group(1).strip()]
                    }
                },
                {
                    'pattern': r'found\s+all\s+(?:the\s+)?(.+?)(?:\s+in|\s*$)',
                    'confidence': 2,
                    'extract': lambda m: {
                        'text': '<from_context>',
                        'extract_type': 'all',
                        'keywords': [m.group(1).strip()]
                    }
                },
                {
                    'pattern': r'extracted?\s+(.+?)\s+from',
                    'confidence': 2,
                    'extract': lambda m: {
                        'text': '<from_context>',
                        'extract_type': 'extract',
                        'keywords': [m.group(1).strip()]
                    }
                },
            ],
            
            'deduplicate_data': [
                {
                    'pattern': r'(?:deduplicated?|removed?\s+duplicates?)',
                    'confidence': 2,
                    'extract': lambda m: {
                        'data': '<from_context>',
                        'key_fields': None
                    }
                },
                {
                    'pattern': r'go\s+through.+?eliminat(?:e|ing)\s+.+?duplicates?',
                    'confidence': 2,
                    'extract': lambda m: {
                        'data': '<from_context>',
                        'key_fields': None
                    }
                },
            ],
            
            'count_occurrences': [
                {
                    'pattern': r'counted?\s+(?:the\s+)?(.+?)(?:\s+from|\s+in|\s*$)',
                    'confidence': 2,
                    'extract': lambda m: {
                        'data': '<from_context>',
                        'target': m.group(1).strip()
                    }
                },
            ],
            
            'find_in_text': [
                {
                    'pattern': r'(?:found|find)\s+["\'](.+?)["\']',
                    'confidence': 2,
                    'extract': lambda m: {
                        'text': '<from_context>',
                        'search_terms': m.group(1)
                    }
                },
                {
                    'pattern': r'find\s+(?:the\s+)?(.+?)(?:\s+label|\s+element)',
                    'confidence': 2,
                    'extract': lambda m: {
                        'text': '<from_context>',
                        'search_terms': m.group(1)
                    }
                },
            ],
            
            'sort_data': [
                {
                    'pattern': r'sort(?:ed)?\s+(?:the\s+)?(?:data|rows?)\s+by\s+(.+?)(?:\s+|$)',
                    'confidence': 2,
                    'extract': lambda m: {
                        'data': '<from_context>',
                        'sort_by': m.group(1).strip()
                    }
                },
            ],
            
            'aggregate_data': [
                {
                    'pattern': r'aggregat(?:ed?|ing)\s+(?:the\s+)?data',
                    'confidence': 2,
                    'extract': lambda m: {
                        'data': '<from_context>',
                        'group_by': '<infer>',
                        'aggregate_field': '<infer>'
                    }
                },
            ],
        }
    
    def _clean_expression(self, expr: str) -> str:

        # ç§»é™¤ç™¾åˆ†è™Ÿ
        expr = expr.replace('%', '/100')
        # ç§»é™¤å¤šé¤˜ç©ºæ ¼
        expr = re.sub(r'\s+', '', expr)
        return expr
    
    def extract_tools(self, text: str) -> List[Tuple[str, Dict, int]]:

        results = []
        
        for tool_name, patterns in self.rules.items():
            for rule in patterns:
                pattern = rule['pattern']
                confidence = rule['confidence']
                extract_func = rule['extract']
                
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    try:
                        arguments = extract_func(match)
                        if arguments:  # æŸäº› extract å¯èƒ½è¿”å› None
                            results.append((tool_name, arguments, confidence))
                            break  # åªå–ç¬¬ä¸€å€‹åŒ¹é…
                    except Exception as e:
                        continue
        
        return results


# ============================================================
# Phase 5: åƒæ•¸é©—è­‰å™¨
# ============================================================

class ParameterValidatorV5:
    
    def __init__(self, data_dir: Path):
        self.data_dir = data_dir
        self.utils = ValidationUtilsV5
    
    def validate_step(self, tool_name: str, arguments: Dict[str, Any]) -> Tuple[bool, List[str]]:

        errors = []
        
        # æª¢æŸ¥æ‰€æœ‰åƒæ•¸æ˜¯å¦åŒ…å« placeholder
        # ç‰¹æ®Šæƒ…æ³ï¼šunit_converter çš„ unit_type å…è¨± <infer>ï¼ˆç¨å¾Œæœƒè‡ªå‹•æ¨æ–·ï¼‰
        for key, value in arguments.items():
            # unit_converter çš„ unit_type å…è¨± <infer>
            if tool_name == 'unit_converter' and key == 'unit_type' and value == '<infer>':
                continue  # è·³éï¼Œç¨å¾Œæœƒè™•ç†
            
            if self.utils.is_placeholder(value):
                errors.append(f"åƒæ•¸ {key} åŒ…å« placeholder: {value}")
        
        # é‡å°ç‰¹å®šå·¥å…·çš„é©—è­‰
        if tool_name == 'web_fetch':
            url = arguments.get('url', '')
            if not self.utils.is_valid_url(url):
                errors.append(f"ç„¡æ•ˆçš„ URL: {url}")
        
        elif tool_name in ['read_pdf', 'read_csv', 'read_excel', 'read_json', 'read_xml', 'read_image']:
            file_path = arguments.get('file_path', '')
            if not self.utils.is_valid_file_path(file_path, self.data_dir):
                errors.append(f"æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
        
        elif tool_name == 'calculate':
            expression = arguments.get('expression', '')
            cleaned = self.utils.clean_calculate_expression(expression)
            if cleaned is None:
                errors.append(f"ç„¡æ³•æ¸…ç†çš„è¡¨é”å¼: {expression}")
            else:
                # æ›´æ–°è¡¨é”å¼ç‚ºæ¸…ç†å¾Œçš„ç‰ˆæœ¬
                arguments['expression'] = cleaned
        
        elif tool_name == 'unit_converter':
            # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤çš„åƒæ•¸ï¼ˆv2.1 çš„ bugï¼‰
            invalid_params = {'operation', 'value1', 'value2', 'result', 'expression'}
            if any(p in arguments for p in invalid_params):
                errors.append(f"åŒ…å«ç„¡æ•ˆåƒæ•¸: {', '.join(invalid_params & arguments.keys())}")
                # é€™äº›éŒ¯èª¤æ˜¯è‡´å‘½çš„ï¼Œç›´æ¥è¿”å›
                return False, errors
            
            # Phase 1.5 å„ªåŒ–ï¼šæ”¾å¯¬åƒæ•¸æª¢æŸ¥ï¼Œå…è¨±éƒ¨åˆ†åƒæ•¸ç¼ºå¤±ä¸¦ä½¿ç”¨ fallback
            # åªæœ‰ç•¶åƒæ•¸ã€Œå®Œå…¨ä¸å­˜åœ¨ã€ä¸”ã€Œç„¡æ³•æ¨æ–·ã€æ™‚æ‰æ‹’çµ•
            
            # æª¢æŸ¥ valueï¼ˆæœ€é—œéµï¼‰
            has_value = 'value' in arguments and arguments['value'] != '<infer>'
            
            # æª¢æŸ¥ from_unit å’Œ to_unit
            has_from = 'from_unit' in arguments and arguments['from_unit'] != '<infer>'
            has_to = 'to_unit' in arguments and arguments['to_unit'] != '<infer>'
            
            # Phase 1.5 å¯¬é¬†ç­–ç•¥ï¼š
            # å¦‚æœè‡³å°‘æœ‰ value å’Œ å…¶ä¸­ä¸€å€‹å–®ä½ï¼Œå°±å˜—è©¦åŸ·è¡Œ
            if not has_value:
                errors.append("ç¼ºå°‘ value åƒæ•¸ä¸”ç„¡æ³•æ¨æ–·")
                return False, errors
            
            # å¦‚æœç¼ºå°‘ from_unitï¼Œå˜—è©¦å¾ä¸Šä¸‹æ–‡æ¨æ–·æˆ–ä½¿ç”¨é è¨­
            if not has_from:
                # æª¢æŸ¥æè¿°æ–‡å­—æ˜¯å¦åŒ…å«å–®ä½ç·šç´¢
                description = arguments.get('description', '')
                # å¸¸è¦‹å–®ä½æ¨¡å¼
                common_units = ['kg', 'g', 'lb', 'm', 'cm', 'km', 'l', 'ml', 'c', 'f', 'k']
                found_unit = None
                for unit in common_units:
                    if unit in description.lower():
                        found_unit = unit
                        break
                
                if found_unit:
                    arguments['from_unit'] = found_unit
                    has_from = True
                else:
                    # ä»ç„¶ç¼ºå¤±ï¼Œä½†ä¸ç«‹å³æ‹’çµ•ï¼Œæ¨™è¨˜è­¦å‘Š
                    pass
            
            # å¦‚æœç¼ºå°‘ to_unitï¼ŒåŒæ¨£å˜—è©¦æ¨æ–·
            if not has_to:
                description = arguments.get('description', '')
                # æŸ¥æ‰¾ "to X" æ¨¡å¼
                import re
                to_pattern = re.search(r'to\s+([a-zA-Z]+)', description, re.IGNORECASE)
                if to_pattern:
                    arguments['to_unit'] = to_pattern.group(1)
                    has_to = True
            
            # æœ€çµ‚æª¢æŸ¥ï¼šè‡³å°‘éœ€è¦ from æˆ– to å…¶ä¸­ä¹‹ä¸€
            if not has_from and not has_to:
                errors.append("ç¼ºå°‘ from_unit å’Œ to_unitï¼Œç„¡æ³•åŸ·è¡Œè½‰æ›")
                return False, errors
            
            # å¦‚æœåªæœ‰ä¸€å€‹å–®ä½ï¼Œè¨˜éŒ„è­¦å‘Šä½†ä»å˜—è©¦
            if not has_from:
                errors.append("è­¦å‘Š: ç¼ºå°‘ from_unitï¼Œå¯èƒ½åŸ·è¡Œå¤±æ•—")
            if not has_to:
                errors.append("è­¦å‘Š: ç¼ºå°‘ to_unitï¼Œå¯èƒ½åŸ·è¡Œå¤±æ•—")
            
            # è™•ç† unit_type
            unit_type = arguments.get('unit_type', 'length')
            
            # å¦‚æœæ˜¯ <infer>ï¼Œå˜—è©¦è‡ªå‹•æ¨æ–·
            if unit_type == '<infer>':
                from_unit = arguments.get('from_unit', '')
                to_unit = arguments.get('to_unit', '')
                unit_type = self.utils.infer_unit_type(from_unit, to_unit)
                arguments['unit_type'] = unit_type
            
            # Phase 1 å„ªåŒ–ï¼šå° pressure æ¡å–å¯¬å®¹æ…‹åº¦ï¼Œè®“å®ƒå˜—è©¦åŸ·è¡Œ
            if unit_type == 'pressure':
                # ä½¿ç”¨ 'length' ä½œç‚º fallbackï¼ˆæœ‰äº›å‡½æ•¸å¯èƒ½æ¥å—ï¼‰
                arguments['unit_type'] = 'length'
            elif unit_type not in VALID_UNIT_TYPES:
                # ä¸æ”¯æ´çš„é¡å‹ï¼Œä½¿ç”¨ length ä½œç‚º fallback
                arguments['unit_type'] = 'length'
        
        elif tool_name in ['extract_information', 'filter_data', 'find_in_text', 'count_occurrences']:
            # é€™äº›å·¥å…·éœ€è¦å¾ä¸Šä¸‹æ–‡ç²å–è³‡æ–™
            data_param = arguments.get('data') or arguments.get('text')
            if self.utils.is_placeholder(data_param):
                errors.append(f"è³‡æ–™åƒæ•¸æ˜¯ placeholderï¼Œéœ€è¦å‰ç½®æ­¥é©Ÿ")
        
        return len(errors) == 0, errors
    
    def fix_parameters(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """ä¿®æ­£åƒæ•¸ï¼ˆæ‡‰ç”¨æ˜ å°„è¦å‰‡ï¼‰"""
        if tool_name not in PARAM_NAME_MAPPING:
            return arguments
        
        mapping = PARAM_NAME_MAPPING[tool_name]
        fixed_args = {}
        
        for key, value in arguments.items():
            new_key = mapping.get(key, key)
            fixed_args[new_key] = value
        
        return fixed_args


# ============================================================
# Phase 6: ZIP è™•ç†å™¨ (from v3.2)
# ============================================================

class ZipHandlerV5:

    def __init__(self, data_dir: Path):
        self.data_dir = data_dir
    
    def should_extract(self, task: Dict) -> bool:
        """åˆ¤æ–·æ˜¯å¦éœ€è¦è§£å£“ ZIP"""
        file_name = task.get('file_name', '')
        return file_name.endswith('.zip')
    
    def create_extract_step(self, task: Dict) -> ParsedStep:
        """å‰µå»º extract_zip æ­¥é©Ÿ"""
        file_name = task.get('file_name', '')
        zip_path = str(self.data_dir / file_name)
        
        return ParsedStep(
            step_number=0,
            original_text=f"Extract {file_name} before processing",
            tool_name='extract_zip',
            arguments={'zip_path': zip_path},
            confidence=3,
            intent_category='file_operation',
            extraction_method='auto_zip_handler',
            executable=True,
            skip_reason=None
        )


# ============================================================
# Phase 7: ä¸» Parser é¡åˆ¥
# ============================================================

class GAIAParserV5Ultimate:
    
    def __init__(self, tasks_file: str, data_dir: str = './data'):
        self._print_header()
        
        # è¼‰å…¥ä»»å‹™
        print(f"\nğŸ“‚ è¼‰å…¥ä»»å‹™: {tasks_file}")
        with open(tasks_file, 'r', encoding='utf-8') as f:
            self.tasks = json.load(f)
        print(f"   âœ… {len(self.tasks)} å€‹ä»»å‹™")
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.data_dir = Path(data_dir)
        self.file_mapper = FileMapperV5(self.tasks, self.data_dir)
        self.extractor = ToolExtractorV5()
        self.validator = ParameterValidatorV5(self.data_dir)
        self.zip_handler = ZipHandlerV5(self.data_dir)
        self.reasoning_filter = ReasoningFilterV5
        
        # çµ±è¨ˆ
        self.stats = {
            'total_tasks': len(self.tasks),
            'steps_extracted': 0,
            'steps_executable': 0,
            'steps_skipped': 0,
            'steps_reasoning': 0,
            'zip_added': 0,
        }
    
    def _print_header(self):
        """æ‰“å°æ¨™é¡Œ"""
        print("=" * 80)
        print("ğŸš€ GAIA Parser v5.0 Ultimate - çµ‚æ¥µæ•´åˆç‰ˆæœ¬")
        print("=" * 80)
        print("\nâœ… æ•´åˆåŠŸèƒ½:")
        print("  â€¢ v2.1 å®Œæ•´å·¥å…·æå–ï¼ˆ43 ç¨®å·¥å…·ï¼‰")
        print("  â€¢ v2.1 æ¨ç†éæ¿¾å™¨")
        print("  â€¢ v2.1 ä¸Šä¸‹æ–‡è¿½è¹¤")
        print("  â€¢ v3 é©—è­‰é‚è¼¯ï¼ˆplaceholder, URL, æª”æ¡ˆï¼‰")
        print("  â€¢ v3.1 åƒæ•¸ä¿®æ­£")
        print("  â€¢ v3.2 è‡ªå‹•ä¿®å¾©ï¼ˆZIP, unit_type, URLï¼‰")
    
    def parse_task(self, task: Dict) -> Dict:
        """è§£æå–®ä¸€ä»»å‹™"""
        task_id = task['task_id']
        steps_text = task.get('Annotator Metadata', {}).get('Steps', '')
        
        if not steps_text:
            return self._empty_result(task)
        
        # è§£ææ­¥é©Ÿ - ä½¿ç”¨ v2.1 çš„åˆ†å‰²æ–¹å¼
        steps = re.split(r'\d+\.\s+', steps_text)
        steps = [s.strip() for s in steps if s.strip()]
        
        parsed_steps = []
        context = ParsingContext()
        
        for i, step_text in enumerate(steps, 1):
            # è·³éç©ºæ­¥é©Ÿ
            if not step_text:
                continue
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºæ¨ç†æ­¥é©Ÿ
            if self.reasoning_filter.is_reasoning(step_text):
                step = ParsedStep(
                    step_number=i,
                    original_text=step_text,
                    tool_name=None,
                    arguments={},
                    confidence=0,
                    intent_category='reasoning',
                    extraction_method='reasoning_filter',
                    is_reasoning=True,
                    executable=False,
                    skip_reason='ç´”æ¨ç†æ­¥é©Ÿ'
                )
                parsed_steps.append(step)
                self.stats['steps_reasoning'] += 1
                continue
            
            # æå–å·¥å…·
            tool_matches = self.extractor.extract_tools(step_text)
            
            if not tool_matches:
                # ç„¡æ³•æå–å·¥å…·
                step = ParsedStep(
                    step_number=i,
                    original_text=step_text,
                    tool_name=None,
                    arguments={},
                    confidence=0,
                    intent_category='unknown',
                    extraction_method='none',
                    executable=False,
                    skip_reason='ç„¡æ³•è­˜åˆ¥å·¥å…·'
                )
                parsed_steps.append(step)
                self.stats['steps_skipped'] += 1
                continue
            
            # å–ä¿¡å¿ƒåº¦æœ€é«˜çš„å·¥å…·
            tool_name, arguments, confidence = max(tool_matches, key=lambda x: x[2])
            
            # ä¿®æ­£åƒæ•¸åç¨±
            arguments = self.validator.fix_parameters(tool_name, arguments)
            
            # ä¿®æ­£æª”æ¡ˆè·¯å¾‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if 'file_path' in arguments:
                file_path = self.file_mapper.get_file_path(task_id)
                if file_path:
                    arguments['file_path'] = file_path
            
            # ä¿®æ­£ URLï¼ˆå¦‚æœæ˜¯ Wikipediaï¼‰
            if tool_name == 'web_fetch' and 'url' in arguments:
                url = arguments['url']
                if 'wikipedia.org' in url:
                    arguments['url'] = ValidationUtilsV5.fix_wikipedia_url(url)
            
            # é©—è­‰
            is_valid, errors = self.validator.validate_step(tool_name, arguments)
            
            step = ParsedStep(
                step_number=i,
                original_text=step_text,
                tool_name=tool_name,
                arguments=arguments,
                confidence=confidence,
                intent_category='tool_call',
                extraction_method='pattern_matching',
                executable=is_valid,
                skip_reason='; '.join(errors) if not is_valid else None
            )
            
            parsed_steps.append(step)
            
            if is_valid:
                self.stats['steps_executable'] += 1
            else:
                self.stats['steps_skipped'] += 1
            
            self.stats['steps_extracted'] += 1
        
        # è™•ç† ZIP
        if self.zip_handler.should_extract(task):
            zip_step = self.zip_handler.create_extract_step(task)
            parsed_steps.insert(0, zip_step)
            self.stats['steps_executable'] += 1
            self.stats['zip_added'] += 1
        
        # è½‰æ›ç‚ºè¼¸å‡ºæ ¼å¼
        tool_sequence = [s.to_dict() for s in parsed_steps]
        executable_count = sum(1 for s in parsed_steps if s.executable)
        
        return {
            'task_id': task_id,
            'question': task.get('Question', ''),
            'final_answer': task.get('Final answer', ''),
            'file_name': task.get('file_name', ''),
            'tool_sequence': tool_sequence,
            'stats': {
                'total_steps': len(parsed_steps),
                'executable_steps': executable_count,
                'skipped_steps': len(parsed_steps) - executable_count,
                'executable_rate': f"{executable_count/len(parsed_steps)*100:.1f}%" if parsed_steps else "0.0%"
            }
        }
    
    def _empty_result(self, task: Dict) -> Dict:
        """ç©ºçµæœ"""
        return {
            'task_id': task['task_id'],
            'question': task.get('Question', ''),
            'final_answer': task.get('Final answer', ''),
            'file_name': task.get('file_name', ''),
            'tool_sequence': [],
            'stats': {
                'total_steps': 0,
                'executable_steps': 0,
                'skipped_steps': 0,
                'executable_rate': "0.0%"
            }
        }
    
    def parse_all(self) -> str:
        """è§£ææ‰€æœ‰ä»»å‹™"""
        print("\n" + "=" * 80)
        print("ğŸ”§ é–‹å§‹è™•ç†")
        print("=" * 80)
        
        results = []
        
        for task in self.tasks:
            task_id = task['task_id']
            print(f"\nè™•ç†: {task_id}")
            
            result = self.parse_task(task)
            results.append(result)
            
            # é¡¯ç¤ºçµ±è¨ˆ
            stats = result['stats']
            if stats['total_steps'] > 0:
                print(f" âœ… {stats['executable_steps']}/{stats['total_steps']} ({stats['executable_rate']})")
            else:
                print(f"ç„¡æ­¥é©Ÿ")
        
        # å„²å­˜çµæœ
        output_dir = Path('./parser_output')
        output_dir.mkdir(exist_ok=True)
        output_file = output_dir / 'plans.json'
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°æœ€çµ‚çµ±è¨ˆ
        self._print_final_stats(results)
        
        print(f"\nâœ… å„²å­˜è‡³: {output_file}")
        return str(output_file)
    
    def _print_final_stats(self, results: List[Dict]):
        """æ‰“å°æœ€çµ‚çµ±è¨ˆ"""
        print("\n" + "=" * 80)
        print("ğŸ“Š æœ€çµ‚çµ±è¨ˆ")
        print("=" * 80)
        
        total_steps = sum(r['stats']['total_steps'] for r in results)
        executable_steps = sum(r['stats']['executable_steps'] for r in results)
        skipped_steps = sum(r['stats']['skipped_steps'] for r in results)
        
        print(f"\nä»»å‹™æ•¸: {self.stats['total_tasks']}")
        print(f"æå–æ­¥é©Ÿ: {self.stats['steps_extracted']}")
        print(f"æ¨ç†æ­¥é©Ÿ: {self.stats['steps_reasoning']} (å·²éæ¿¾)")
        print(f"å¯åŸ·è¡Œæ­¥é©Ÿ: {executable_steps} ({executable_steps/total_steps*100:.1f}%)" if total_steps > 0 else "å¯åŸ·è¡Œæ­¥é©Ÿ: 0")
        print(f"è·³éæ­¥é©Ÿ: {skipped_steps}")
        print(f"ZIP è‡ªå‹•è™•ç†: {self.stats['zip_added']} å€‹")
        
        # å„å·¥å…·çµ±è¨ˆ
        tool_counts = defaultdict(int)
        for result in results:
            for step in result['tool_sequence']:
                tool_name = step.get('tool_name')
                if tool_name:
                    tool_counts[tool_name] += 1
        
        print(f"\nå·¥å…·ä½¿ç”¨çµ±è¨ˆ:")
        for tool_name, count in sorted(tool_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f" â€¢ {tool_name}: {count}")

def main():
    import sys
    
    # æª¢æŸ¥åƒæ•¸
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹å¼: python3 parser_v5_ultimate.py <tasks_file> [data_dir]")
        print("ç¯„ä¾‹: python3 parser_v5_ultimate.py gaia_level3_tasks.json ./data")
        sys.exit(1)
    
    tasks_file = sys.argv[1]
    data_dir = sys.argv[2] if len(sys.argv) > 2 else './data'
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not Path(tasks_file).exists():
        print(f"æ‰¾ä¸åˆ°ä»»å‹™æª”æ¡ˆ: {tasks_file}")
        sys.exit(1)
    
    # åŸ·è¡Œè§£æ
    parser = GAIAParserV5Ultimate(tasks_file, data_dir)
    output_file = parser.parse_all()
    
if __name__ == '__main__':
    main()

