#!/usr/bin/env python3
"""
è‡ªå‹•ç­”æ¡ˆé©—è­‰ç‰ˆæœ¬
"""
import os
import sys
import json
import re

# ä½¿ç”¨ç•¶å‰ç›®éŒ„
PROJECT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_DIR)

# è¼‰å…¥è³‡æ–™
with open(os.path.join(PROJECT_DIR, 'gaia_level3_tasks.json'), 'r', encoding='utf-8') as f:
    tasks = json.load(f)

exec_results_file = os.path.join(PROJECT_DIR, 'parser_output/execution_results.json')
if not os.path.exists(exec_results_file):
    print("âš ï¸  åŸ·è¡Œçµæœæª”æ¡ˆä¸å­˜åœ¨ï¼Œè·³éé©—è­‰")
    sys.exit(0)

with open(exec_results_file, 'r', encoding='utf-8') as f:
    execution_results = json.load(f)

# å»ºç«‹ task_id -> execution_result æ˜ å°„
exec_map = {r['task_id']: r for r in execution_results}

print("="*80)
print("ğŸ“Š ç­”æ¡ˆé©—è­‰çµæœ")
print("="*80)

validation_results = []

for task in tasks:
    task_id = task['task_id']
    ground_truth = task['Final answer']
    question = task['Question']
    
    exec_result = exec_map.get(task_id)
    
    print(f"\n{'='*80}")
    print(f"ä»»å‹™: {task_id}")
    print(f"æ¨™æº–ç­”æ¡ˆ: {ground_truth}")
    print('='*80)
    
    # åˆ†æç­”æ¡ˆé¡å‹
    answer_type = 'text'
    if re.match(r'^\d+$', ground_truth):
        answer_type = 'integer'
    elif re.match(r'^\d+\.\d+$', ground_truth):
        answer_type = 'float'
    
    print(f"ç­”æ¡ˆé¡å‹: {answer_type}")
    
    # å˜—è©¦å¾åŸ·è¡Œçµæœä¸­æå–ç­”æ¡ˆ
    predicted_answer = None
    validation_status = 'unknown'
    
    if exec_result and exec_result['results']:
        # æª¢æŸ¥æœ€å¾Œä¸€æ­¥æ˜¯å¦æ˜¯è¨ˆç®—
        last_result = exec_result['results'][-1]
        if last_result['success'] and last_result.get('result'):
            result_data = last_result['result']
            
            # å¦‚æœæ˜¯ calculate å·¥å…·
            if last_result['tool_name'] == 'calculate':
                if 'result' in result_data:
                    predicted_answer = str(result_data['result'])
                    
                    # æ¯”å°ç­”æ¡ˆ
                    if answer_type == 'integer':
                        try:
                            pred_int = int(float(predicted_answer))
                            truth_int = int(ground_truth)
                            if pred_int == truth_int:
                                validation_status = 'correct'
                            else:
                                validation_status = 'incorrect'
                        except:
                            validation_status = 'unknown'
                    elif answer_type == 'float':
                        try:
                            pred_float = float(predicted_answer)
                            truth_float = float(ground_truth)
                            if abs(pred_float - truth_float) < 0.1:
                                validation_status = 'correct'
                            else:
                                validation_status = 'incorrect'
                        except:
                            validation_status = 'unknown'
    
    print(f"é æ¸¬ç­”æ¡ˆ: {predicted_answer if predicted_answer else 'N/A'}")
    print(f"é©—è­‰ç‹€æ…‹: {validation_status}")
    
    if validation_status == 'correct':
        print("âœ… ç­”æ¡ˆæ­£ç¢º")
    elif validation_status == 'incorrect':
        print("âŒ ç­”æ¡ˆéŒ¯èª¤")
    else:
        print("âš ï¸  éœ€è¦äººå·¥é©—è­‰")
    
    validation_results.append({
        'task_id': task_id,
        'ground_truth': ground_truth,
        'predicted_answer': predicted_answer,
        'answer_type': answer_type,
        'validation_status': validation_status,
        'success_rate': exec_result['success_rate'] if exec_result else 0
    })

# çµ±è¨ˆ
print("\n" + "="*80)
print("ğŸ“ˆ æ•´é«”çµ±è¨ˆ")
print("="*80)

correct = sum(1 for v in validation_results if v['validation_status'] == 'correct')
incorrect = sum(1 for v in validation_results if v['validation_status'] == 'incorrect')
unknown = sum(1 for v in validation_results if v['validation_status'] == 'unknown')

print(f"\nç­”æ¡ˆé©—è­‰:")
print(f"  æ­£ç¢º: {correct}/{len(validation_results)} ({correct/len(validation_results)*100:.1f}%)")
print(f"  éŒ¯èª¤: {incorrect}/{len(validation_results)}")
print(f"  æœªçŸ¥: {unknown}/{len(validation_results)}")

# å„²å­˜çµæœ
output_file = os.path.join(PROJECT_DIR, 'parser_output/validation_results.json')
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(validation_results, f, indent=2, ensure_ascii=False)

print(f"\nâœ… é©—è­‰çµæœå·²å„²å­˜è‡³: {output_file}")
